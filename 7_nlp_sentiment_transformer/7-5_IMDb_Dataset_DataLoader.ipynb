{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 IMDb（Internet Movie Database）からDataLoaderを作成\n",
    "\n",
    "- 本ファイルでは、IMDb（Internet Movie Database）のデータを使用して、感情分析（0：ネガティブ、1：ポジティブ）を2値クラス分類するためのDatasetとDataLoaderを作成します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 学習目標\n",
    "\n",
    "1.\tテキスト形式のファイルデータからtsvファイルを作成し、torchtext用のDataLoaderを作成できるようになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "書籍の指示に従い、本章で使用するデータを用意します\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMDbデータセットをtsv形式に変換\n",
    "\n",
    "Datasetをダウンロードします\n",
    "\n",
    "※torchtextで標準でIMDbが使える関数があるのですが、今回は今後データセットが用意されていない場合でも対応できるように0から作ります。\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "5万件のデータ（train,testともに2.5万件）です。データidとrating（1-10）でファイル名が決まっています。\n",
    "\n",
    "rateは10の方が良いです。4以下がnegative、7以上がpositiveにクラス分けされています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsv形式のファイルにします\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "\n",
    "\n",
    "# 訓練データのtsvファイルを作成します\n",
    "\n",
    "f = open('./data/IMDb_train.tsv', 'w')\n",
    "\n",
    "path = './data/aclImdb/train/pos/'\n",
    "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消しておきます\n",
    "        text = text.replace('\\t', \" \")\n",
    "\n",
    "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "        f.write(text)\n",
    "\n",
    "path = './data/aclImdb/train/neg/'\n",
    "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消しておきます\n",
    "        text = text.replace('\\t', \" \")\n",
    "\n",
    "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "        f.write(text)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの作成\n",
    "\n",
    "f = open('./data/IMDb_test.tsv', 'w')\n",
    "\n",
    "path = './data/aclImdb/test/pos/'\n",
    "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消しておきます\n",
    "        text = text.replace('\\t', \" \")\n",
    "\n",
    "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "path = './data/aclImdb/test/neg/'\n",
    "\n",
    "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "        text = ff.readline()\n",
    "\n",
    "        # タブがあれば消しておきます\n",
    "        text = text.replace('\\t', \" \")\n",
    "\n",
    "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "        f.write(text)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 前処理と単語分割の関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# 以下の記号はスペースに置き換えます（カンマ、ピリオドを除く）。\n",
    "# punctuationとは日本語で句点という意味です\n",
    "print(\"区切り文字：\", string.punctuation)\n",
    "# !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "# 前処理\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "\n",
    "\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# 動作を確認します\n",
    "print(tokenizer_with_preprocessing('I like cats.'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "import torchtext\n",
    "\n",
    "\n",
    "# 文章とラベルの両方に用意します\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# 引数の意味は次の通り\n",
    "# init_token：全部の文章で、文頭に入れておく単語\n",
    "# eos_token：全部の文章で、文末に入れておく単語\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='IMDb_train.tsv',\n",
    "    test='IMDb_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "# 動作確認\n",
    "print('訓練および検証のデータ数', len(train_val_ds))\n",
    "print('1つ目の訓練および検証のデータ', vars(train_val_ds[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(\n",
    "    split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "# 動作確認\n",
    "print('訓練データの数', len(train_ds))\n",
    "print('検証データの数', len(val_ds))\n",
    "print('1つ目の訓練データ', vars(train_ds[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボキャブラリーを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "english_fasttext_vectors = Vectors(name='data/wiki-news-300d-1M.vec')\n",
    "\n",
    "\n",
    "# 単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", english_fasttext_vectors.dim)\n",
    "print(\"単語数：\", len(english_fasttext_vectors.itos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "# ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape)  # 17916個の単語が300次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n",
    "\n",
    "# ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=24, train=True)\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=24, train=False, sort=False)\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=24, train=False, sort=False)\n",
    "\n",
    "\n",
    "# 動作確認 検証データのデータセットで確認\n",
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにDataLoaderは単語のidを格納しているので、分散表現はディープラーニングモデル側でidに応じて取得してあげる必要があります。\n",
    "\n",
    "ここまでの内容をフォルダ「utils」のdataloader.pyに別途保存しておき、次節からはこちらから読み込むようにします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
