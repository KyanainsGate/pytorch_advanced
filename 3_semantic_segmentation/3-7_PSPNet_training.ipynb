{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7fOv9Gc5Dkg"
   },
   "source": [
    "# 3.7 学習と検証の実施\n",
    "\n",
    "- 本ファイルでは、PSPNetの学習と検証の実施を行います。AWSのGPUマシンで計算します。\n",
    "- p2.xlargeで約12時間かかります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYZ3dHjo5Dkh"
   },
   "source": [
    "# 学習目標\n",
    "\n",
    "1.\tPSPNetの学習と検証を実装できるようになる\n",
    "2.\tセマンティックセグメンテーションのファインチューニングを理解する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kCZ8pIO5Dki"
   },
   "source": [
    "# 事前準備\n",
    "\n",
    "- 本書に従い学習済みモデルのファイル「pspnet50_ADE20K.pth」をダウンロードし、フォルダ「weights」に用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aokMFYco5Dki"
   },
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvHo-7O-5Dkl"
   },
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejh1Rqxm5Dko"
   },
   "source": [
    "# DataLoader作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lKxvOS65Dko"
   },
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# ファイルパスリスト作成\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "# (RGB)の色の平均値と標準偏差\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEAUQA9q5Dkr"
   },
   "source": [
    "# ネットワークモデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8AkC5NH5Dkr",
    "outputId": "9d661cec-bbc4-455c-ff02-9e6639385f11"
   },
   "outputs": [],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ファインチューニングでPSPNetを作成\n",
    "# ADE20Kデータセットの学習済みモデルを使用、ADE20Kはクラス数が150です\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "# ADE20K学習済みパラメータをロード\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 分類用の畳み込み層を、出力数21のものにつけかえる\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(\n",
    "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "net.aux.classification = nn.Conv2d(\n",
    "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する。\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)\n",
    "\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXuKq8VP5Dku",
    "outputId": "7257e247-3e56-4f49-e72c-8ceb68357433"
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5bYr52Z5Dkw"
   },
   "source": [
    "# 損失関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0Y6_cZh5Dkx"
   },
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "class PSPLoss(nn.Module):\n",
    "    \"\"\"PSPNetの損失関数のクラスです。\"\"\"\n",
    "\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight  # aux_lossの重み\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : PSPNetの出力(tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
    "\n",
    "        targets : [num_batch, 475, 475]\n",
    "            正解のアノテーション情報\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : テンソル\n",
    "            損失の値\n",
    "        \"\"\"\n",
    "\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gevOccLt5Dkz"
   },
   "source": [
    "# 最適化手法を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HQuH4465Dk0"
   },
   "outputs": [],
   "source": [
    "# ファインチューニングなので、学習率は小さく\n",
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "# スケジューラーの設定\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0C1CqUP5Dk3"
   },
   "source": [
    "# 学習・検証を実施する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDo8kP3r5Dk3"
   },
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                scheduler.step()  # 最適化schedulerの更新\n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "\n",
    "                \n",
    "                # multiple minibatchでのパラメータの更新\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    # 最後のネットワークを保存する\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XEy-KBb5Dk5",
    "outputId": "9f1b844f-178f-45f0-80fb-b992b8b452dc"
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する\n",
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXHjujLZ5Dk8"
   },
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3-7_PSPNet_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
