{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBGEws_BNH2P"
   },
   "source": [
    "# 1.1「学習済みVGGモデル」を使用する方法\n",
    "\n",
    "- 本ファイルでは、学習済みのVGGモデルを使用し、未知の画像（ゴールデンレトリバー）を分類します\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBfSv2DyNH2W"
   },
   "source": [
    "# 学習目標\n",
    "\n",
    "1.\tPyTorchでImangeNetデータセットでの学習済みモデルをロードできるようになる\n",
    "2.\tVGGモデルについて理解する\n",
    "3.\t入力画像のサイズや色を変換できるようになる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6H1SGoeNH2X"
   },
   "source": [
    "# 事前準備\n",
    "\n",
    "\n",
    "1. 書籍の指示に従い、make_folders_and_data_downloads.ipynbを実行して、本章で使用するデータをダウンロード\n",
    "\n",
    "\n",
    "2. PyTorchのインストールページ（ https://pytorch.org/get-started/locally/ ）を参考に、PyTorch1.0をインストール\n",
    "\n",
    "\n",
    "conda install pytorch-cpu torchvision-cpu -c pytorch\n",
    "\n",
    "（Windowsで、GPUなしの環境をcondaでインストールする場合）\n",
    "\n",
    "\n",
    "3. Matplotlibをインストール\n",
    "\n",
    "conda install -c conda-forge matplotlib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFizVjJC2ru5"
   },
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioNjJoeW2qMA",
    "outputId": "0f0f6d54-92c8-411b-fdb2-5ea3a79c09e6"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/YutaroOgawa/pytorch_advanced.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBC5_RkU2rDD",
    "outputId": "75b6f8bc-cce2-4002-bb11-c356255d2908"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdQ0Tl3-2_Dq",
    "outputId": "3ccd0dbb-8a29-448b-c4b8-afdf7d00bd36"
   },
   "outputs": [],
   "source": [
    "%cd \"pytorch_advanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1eUhXws2_U6",
    "outputId": "df5e7d66-4a06-4120-c0d9-71a9a67aa820"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0uT1OSr20a6",
    "outputId": "61fab8a6-1be1-4cae-9474-536837652c92"
   },
   "outputs": [],
   "source": [
    "%cd \"1_image_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpfRDZve26fv",
    "outputId": "f2f79ddd-32eb-4a50-b019-85055aeaae9a"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KefQTosN3G9_"
   },
   "outputs": [],
   "source": [
    "# make_folders_and_data_downloads.ipynbの中身を実行\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "save_path = os.path.join(data_dir, \"imagenet_class_index.json\")\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "save_path = os.path.join(data_dir, \"hymenoptera_data.zip\")\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "    # ZIPファイルを読み込み\n",
    "    zip = zipfile.ZipFile(save_path)\n",
    "    zip.extractall(data_dir)  # ZIPを解凍\n",
    "    zip.close()  # ZIPファイルをクローズ\n",
    "\n",
    "    # ZIPファイルを消去\n",
    "    os.remove(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9E8R-lQ3HPR"
   },
   "outputs": [],
   "source": [
    "# Colaboratory用の準備完了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpQa-OCQNH2X"
   },
   "source": [
    "# パッケージのimportとPyTorchのバージョンを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTYHPeyGNH2Y"
   },
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWFzRfYwNH2Z",
    "outputId": "fc9a34cf-aa9f-414e-b6e0-5b7795cdb473"
   },
   "outputs": [],
   "source": [
    "# PyTorchのバージョン確認\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQj18-KqNH2Z"
   },
   "source": [
    "# VGG-16の学習済みモデルをロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876,
     "referenced_widgets": [
      "2ec65e8c6d6345dc9950e863b8a2be7f",
      "5376441f9bf94fd9acc6994afff73ebe",
      "313d0579b4734655a1378d329d904c96",
      "a4fbf01a73be4dcc8240abe74a546479",
      "91d3bc40b4274489baba267b73c080a0",
      "33694e05ee4b48caae3309ecc884da4a",
      "6cc7167e04b14bafa404a31e08f3a883",
      "611884d0e3b6487aa9c754e5fe63af8a"
     ]
    },
    "id": "ugoB5bhBNH2a",
    "outputId": "ec30f039-18c1-438d-cf9c-f097899171cd"
   },
   "outputs": [],
   "source": [
    "# 学習済みのVGG-16モデルをロード\n",
    "# 初めて実行する際は、学習済みパラメータをダウンロードするため、実行に時間がかかります\n",
    "\n",
    "# VGG-16モデルのインスタンスを生成\n",
    "use_pretrained = True  # 学習済みのパラメータを使用\n",
    "net = models.vgg16(pretrained=use_pretrained)\n",
    "net.eval()  # 推論モードに設定\n",
    "\n",
    "# モデルのネットワーク構成を出力\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbnucsIQNH2a"
   },
   "source": [
    "# 入力画像の前処理クラスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wegLsyJKNH2a"
   },
   "outputs": [],
   "source": [
    "# 入力画像の前処理のクラス\n",
    "class BaseTransform():\n",
    "    \"\"\"\n",
    "    画像のサイズをリサイズし、色を標準化する。\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    resize : int\n",
    "        リサイズ先の画像の大きさ。\n",
    "    mean : (R, G, B)\n",
    "        各色チャネルの平均値。\n",
    "    std : (R, G, B)\n",
    "        各色チャネルの標準偏差。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize(resize),  # 短い辺の長さがresizeの大きさになる\n",
    "            transforms.CenterCrop(resize),  # 画像中央をresize × resizeで切り取り\n",
    "            transforms.ToTensor(),  # Torchテンソルに変換\n",
    "            transforms.Normalize(mean, std)  # 色情報の標準化\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.base_transform(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "JyPkp-v9NH2b",
    "outputId": "4c6f4812-ff5e-4a0f-e91d-f14980651ad5"
   },
   "outputs": [],
   "source": [
    "# 画像前処理の動作を確認\n",
    "\n",
    "# 1. 画像読み込み\n",
    "image_file_path = './data/goldenretriever-3724972_640.jpg'\n",
    "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
    "\n",
    "# 2. 元の画像の表示\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# 3. 画像の前処理と処理済み画像の表示\n",
    "resize = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "transform = BaseTransform(resize, mean, std)\n",
    "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
    "\n",
    "# (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
    "img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n",
    "img_transformed = np.clip(img_transformed, 0, 1)\n",
    "plt.imshow(img_transformed)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z2ZWxWnNH2b"
   },
   "source": [
    "# 出力結果からラベルを予測する後処理クラスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uWvtv09NH2c",
    "outputId": "001996b7-a6cc-4fd2-90e8-d2524e4d2811"
   },
   "outputs": [],
   "source": [
    "# ILSVRCのラベル情報をロードし辞書型変数を生成します\n",
    "ILSVRC_class_index = json.load(open('./data/imagenet_class_index.json', 'r'))\n",
    "ILSVRC_class_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fGbk8QpNH2c"
   },
   "outputs": [],
   "source": [
    "# 出力結果からラベルを予測する後処理クラス\n",
    "class ILSVRCPredictor():\n",
    "    \"\"\"\n",
    "    ILSVRCデータに対するモデルの出力からラベルを求める。\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    class_index : dictionary\n",
    "            クラスindexとラベル名を対応させた辞書型変数。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_index):\n",
    "        self.class_index = class_index\n",
    "\n",
    "    def predict_max(self, out):\n",
    "        \"\"\"\n",
    "        確率最大のILSVRCのラベル名を取得する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        out : torch.Size([1, 1000])\n",
    "            Netからの出力。\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_label_name : str\n",
    "            最も予測確率が高いラベルの名前\n",
    "        \"\"\"\n",
    "        maxid = np.argmax(out.detach().numpy())\n",
    "        predicted_label_name = self.class_index[str(maxid)][1]\n",
    "\n",
    "        return predicted_label_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMbCCgJvNH2d"
   },
   "source": [
    "# 学習済みVGGモデルで手元の画像を予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EhD_ZQYNH2d",
    "outputId": "dc4ab0ea-fde3-4878-a5df-98e3c52b636f"
   },
   "outputs": [],
   "source": [
    "# ILSVRCのラベル情報をロードし辞意書型変数を生成します\n",
    "ILSVRC_class_index = json.load(open('./data/imagenet_class_index.json', 'r'))\n",
    "\n",
    "# ILSVRCPredictorのインスタンスを生成します\n",
    "predictor = ILSVRCPredictor(ILSVRC_class_index)\n",
    "\n",
    "# 入力画像を読み込む\n",
    "image_file_path = './data/goldenretriever-3724972_640.jpg'\n",
    "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
    "\n",
    "# 前処理の後、バッチサイズの次元を追加する\n",
    "transform = BaseTransform(resize, mean, std)  # 前処理クラス作成\n",
    "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
    "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
    "\n",
    "# モデルに入力し、モデル出力をラベルに変換する\n",
    "out = net(inputs)  # torch.Size([1, 1000])\n",
    "result = predictor.predict_max(out)\n",
    "\n",
    "# 予測結果を出力する\n",
    "print(\"入力画像の予測結果：\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cs8IdOrNH2d"
   },
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1-1_load_vgg_on_GoogleColab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
