{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMMvyHhxGgtt"
   },
   "source": [
    "# 4.7 推論の実施\n",
    "\n",
    "- 本ファイルでは、学習させたOpenPoseで姿勢推定を行います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxd4CMjKGgt1"
   },
   "source": [
    "# 学習目標\n",
    "\n",
    "\n",
    "1.\tOpenPoseの学習済みモデルをロードできるようになる\n",
    "2.\tOpenPoseの推論を実装できるようになる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWPaCzjEGgt2"
   },
   "source": [
    "# 事前準備\n",
    "\n",
    "- 学習済みの重みパラメータ「pose_model_scratch.pth」をフォルダ「weights」に用意する。\n",
    "\n",
    "https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation\n",
    "\n",
    "https://www.dropbox.com/s/5v654d2u65fuvyr/pose_model_scratch.pth?dl=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHMXPJsSGgt2"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg_MjI-OGgt3",
    "outputId": "3cc78f55-03fb-4cf5-9541-cd242324c8b2"
   },
   "outputs": [],
   "source": [
    "from utils.openpose_net import OpenPoseNet\n",
    "\n",
    "# 学習済みモデルと本章のモデルでネットワークの層の名前が違うので、対応させてロードする\n",
    "# モデルの定義\n",
    "net = OpenPoseNet()\n",
    "\n",
    "# 学習済みパラメータをロードする\n",
    "net_weights = torch.load(\n",
    "    './weights/pose_model_scratch.pth', map_location={'cuda:0': 'cpu'})\n",
    "keys = list(net_weights.keys())\n",
    "\n",
    "weights_load = {}\n",
    "\n",
    "# ロードした内容を、本書で構築したモデルの\n",
    "# パラメータ名net.state_dict().keys()にコピーする\n",
    "for i in range(len(keys)):\n",
    "    weights_load[list(net.state_dict().keys())[i]\n",
    "                 ] = net_weights[list(keys)[i]]\n",
    "\n",
    "# コピーした内容をモデルに与える\n",
    "state = net.state_dict()\n",
    "state.update(weights_load)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xX2RJt_Ggt4",
    "outputId": "610eadec-b0e9-4d3e-ca1c-92ff8479e60f"
   },
   "outputs": [],
   "source": [
    "# 草野球の画像を読み込み、前処理します\n",
    "\n",
    "test_image = './data/hit-1407826_640.jpg'\n",
    "oriImg = cv2.imread(test_image)  # B,G,Rの順番\n",
    "\n",
    "# BGRをRGBにして表示\n",
    "oriImg = cv2.cvtColor(oriImg, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(oriImg)\n",
    "plt.show()\n",
    "\n",
    "# 画像のリサイズ\n",
    "size = (368, 368)\n",
    "img = cv2.resize(oriImg, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# 画像の前処理\n",
    "img = img.astype(np.float32) / 255.\n",
    "\n",
    "# 色情報の標準化\n",
    "color_mean = [0.485, 0.456, 0.406]\n",
    "color_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 21/03/07 Issue147 https://github.com/YutaroOgawa/pytorch_advanced/issues/147\n",
    "# 色チャネルの順番を誤っています\n",
    "# preprocessed_img = img.copy()[:, :, ::-1]  # RGB→BGR\n",
    "preprocessed_img = img.copy()  # RGB\n",
    "\n",
    "for i in range(3):\n",
    "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - color_mean[i]\n",
    "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / color_std[i]\n",
    "\n",
    "# （高さ、幅、色）→（色、高さ、幅）\n",
    "img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "# 画像をTensorに\n",
    "img = torch.from_numpy(img)\n",
    "\n",
    "# ミニバッチ化：torch.Size([1, 3, 368, 368])\n",
    "x = img.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRwwD0jTGgt4"
   },
   "outputs": [],
   "source": [
    "# OpenPoseでheatmapsとPAFsを求めます\n",
    "net.eval()\n",
    "predicted_outputs, _ = net(x)\n",
    "\n",
    "# 画像をテンソルからNumPyに変化し、サイズを戻します\n",
    "pafs = predicted_outputs[0][0].detach().numpy().transpose(1, 2, 0)\n",
    "heatmaps = predicted_outputs[1][0].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "pafs = cv2.resize(pafs, size, interpolation=cv2.INTER_CUBIC)\n",
    "heatmaps = cv2.resize(heatmaps, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "pafs = cv2.resize(\n",
    "    pafs, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "heatmaps = cv2.resize(\n",
    "    heatmaps, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd-YecEqGgt5",
    "outputId": "2c992171-dd90-41ac-9332-707161bc0884"
   },
   "outputs": [],
   "source": [
    "# 左肘と左手首のheatmap、そして左肘と左手首をつなぐPAFのxベクトルを可視化する\n",
    "# 左肘\n",
    "heat_map = heatmaps[:, :, 6]  # 6は左肘\n",
    "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
    "heat_map = np.asarray(heat_map.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 左手首\n",
    "heat_map = heatmaps[:, :, 7]  # 7は左手首\n",
    "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
    "heat_map = np.asarray(heat_map.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 左肘と左手首をつなぐPAFのxベクトル\n",
    "paf = pafs[:, :, 24]\n",
    "paf = Image.fromarray(np.uint8(cm.jet(paf)*255))\n",
    "paf = np.asarray(paf.convert('RGB'))\n",
    "\n",
    "# 合成して表示\n",
    "blend_img = cv2.addWeighted(oriImg, 0.5, paf, 0.5, 0)\n",
    "plt.imshow(blend_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhR2IvFEGgt5"
   },
   "outputs": [],
   "source": [
    "from utils.decode_pose import decode_pose\n",
    "_, result_img, _, _ = decode_pose(oriImg, heatmaps, pafs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFC-m6szGgt6",
    "outputId": "5b73a685-3801-4d49-d217-d8ba2afc78b0"
   },
   "outputs": [],
   "source": [
    "# 結果を描画\n",
    "plt.imshow(oriImg)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(result_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5nnUpdLGgt6"
   },
   "source": [
    "以上"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-7_OpenPose_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
